{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import TimeLimit\n",
    "from env_hiv import HIVPatient\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from replay_buffer import ReplayBuffer\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from DQN import DQN\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TimeLimit(\n",
    "    env=HIVPatient(domain_randomization=False), max_episode_steps=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_action(network, state):\n",
    "    device = next(network.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        Q = network(torch.Tensor(state).unsqueeze(0).to(device))\n",
    "        return torch.argmax(Q).item()\n",
    "\n",
    "class ProjectAgent:\n",
    "    def __init__(self, env,model):\n",
    "        self.n_actions = 4\n",
    "        self.state_dim = 6\n",
    "        self.gamma = 0.85 \n",
    "        self.device = \"cuda\" if next(model.parameters()).is_cuda else \"cpu\"\n",
    "        self.save_path = \"agent.pt\"\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer(capacity=60000,device = self.device)\n",
    "        self.model = model\n",
    "        self.lr = 1e-3 \n",
    "        self.batch_size = 1024\n",
    "\n",
    "        self.epsilon_max = 1\n",
    "        self.epsilon_min =  0.01\n",
    "        self.epsilon_stop = 1000\n",
    "        self.epsilon_delay = 20\n",
    "        self.epsilon_step = (self.epsilon_max-self.epsilon_min)/self.epsilon_stop\n",
    "\n",
    "        \n",
    "        self.update_count = 0\n",
    "        self.target_update_freq = 100\n",
    "        self.update_target_tau = 0.005\n",
    "        self.update_target_strategy = 'ema'\n",
    "        self.nb_gradient_steps = 1\n",
    " \n",
    "        #self.q_network = QNetwork(self.state_dim, self.n_actions).to(self.device)\n",
    "        self.target_network = DQN(self.state_dim, self.n_actions).to(self.device)\n",
    "           \n",
    "        self.target_network.load_state_dict(self.model.state_dict())\n",
    "        self.target_network.eval()\n",
    "        \n",
    "        self.monitoring_nb_trials = 4\n",
    "        self.monitor_every =  4\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(),lr=self.lr)                     \n",
    "        #self.scheduler = lr_scheduler.StepLR(self.optimizer, step_size=350, gamma=0.5)\n",
    "    \n",
    "    def MC_eval(self, env, nb_trials):   \n",
    "        MC_total_reward = []\n",
    "        MC_discounted_reward = []\n",
    "        for _ in range(nb_trials):\n",
    "            x,_ = env.reset()\n",
    "            done = False\n",
    "            trunc = False\n",
    "            total_reward = 0\n",
    "            discounted_reward = 0\n",
    "            step = 0\n",
    "            while not (done or trunc):\n",
    "                a = greedy_action(self.model, x)\n",
    "                y,r,done,trunc,_ = env.step(a)\n",
    "                x = y\n",
    "                total_reward += r\n",
    "                discounted_reward += self.gamma**step * r\n",
    "                step += 1\n",
    "            MC_total_reward.append(total_reward)\n",
    "            MC_discounted_reward.append(discounted_reward)\n",
    "        return np.mean(MC_discounted_reward), np.mean(MC_total_reward)\n",
    "\n",
    "    def V_initial_state(self, env, nb_trials):   \n",
    "        with torch.no_grad():\n",
    "            for _ in range(nb_trials):\n",
    "                val = []\n",
    "                x,_ = env.reset()\n",
    "                val.append(self.model(torch.Tensor(x).unsqueeze(0).to(self.device)).max().item())\n",
    "        return np.mean(val)\n",
    "    \n",
    "    def gradient_step(self):\n",
    "        if len(self.replay_buffer) > self.batch_size:\n",
    "            X, A, R, Y, D = self.replay_buffer.sample(self.batch_size)\n",
    "            QYmax = self.target_network(Y).max(1)[0].detach()\n",
    "            update = torch.addcmul(R, 1-D, QYmax, value=self.gamma)\n",
    "            QXA = self.model(X).gather(1, A.to(torch.long).unsqueeze(1))\n",
    "            loss = self.criterion(QXA, update.unsqueeze(1))\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step() \n",
    "\n",
    "    def act(self, observation, use_random=False):\n",
    "        if use_random:\n",
    "            return env.action_space.sample()\n",
    "        else:\n",
    "            return greedy_action(self.model, observation)\n",
    "\n",
    "    def train(self, env, max_episode):\n",
    "        episode_return = []\n",
    "        MC_avg_total_reward = []   # NEW NEW NEW\n",
    "        MC_avg_discounted_reward = []   # NEW NEW NEW\n",
    "        V_init_state = []   # NEW NEW NEW\n",
    "        episode = 0\n",
    "        episode_cum_reward = 0\n",
    "        state, _ = env.reset()\n",
    "        epsilon = self.epsilon_max\n",
    "        step = 0\n",
    "        while episode < max_episode:\n",
    "            # update epsilon\n",
    "            if step > self.epsilon_delay:\n",
    "                epsilon = max(self.epsilon_min, epsilon-self.epsilon_step)\n",
    "            # select epsilon-greedy action\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = greedy_action(self.model, state)\n",
    "            # step\n",
    "            next_state, reward, done, trunc, _ = env.step(action)\n",
    "            self.replay_buffer.append(state, action, reward, next_state, done)\n",
    "            episode_cum_reward += reward\n",
    "            # train\n",
    "            for _ in range(self.nb_gradient_steps): \n",
    "                self.gradient_step()\n",
    "            # update target network if needed\n",
    "            if self.update_target_strategy == 'replace':\n",
    "                if step % self.update_target_freq == 0: \n",
    "                    self.target_network.load_state_dict(self.model.state_dict())\n",
    "            if self.update_target_strategy == 'ema':\n",
    "                target_state_dict = self.target_network.state_dict()\n",
    "                model_state_dict = self.model.state_dict()\n",
    "                tau = self.update_target_tau\n",
    "                for key in model_state_dict:\n",
    "                    target_state_dict[key] = tau*model_state_dict[key] + (1-tau)*target_state_dict[key]\n",
    "                self.target_network.load_state_dict(target_state_dict)\n",
    "            # next transition\n",
    "            step += 1\n",
    "            if done or trunc:\n",
    "                episode += 1\n",
    "                # Monitoring\n",
    "                if self.monitoring_nb_trials>0:\n",
    "                    MC_dr, MC_tr = self.MC_eval(env, self.monitoring_nb_trials)    # NEW NEW NEW\n",
    "                    V0 = self.V_initial_state(env, self.monitoring_nb_trials)   # NEW NEW NEW\n",
    "                    MC_avg_total_reward.append(MC_tr)   # NEW NEW NEW\n",
    "                    MC_avg_discounted_reward.append(MC_dr)   # NEW NEW NEW\n",
    "                    V_init_state.append(V0)   # NEW NEW NEW\n",
    "                    episode_return.append(episode_cum_reward)   # NEW NEW NEW\n",
    "                    print(\"Episode \", '{:2d}'.format(episode), \n",
    "                          \", epsilon \", '{:6.2f}'.format(epsilon), \n",
    "                          \", batch size \", '{:4d}'.format(len(self.replay_buffer)), \n",
    "                          \", ep return \", '{:4.1f}'.format(episode_cum_reward), \n",
    "                          \", MC tot \", '{:6.2f}'.format(MC_tr),\n",
    "                          \", MC disc \", '{:6.2f}'.format(MC_dr),\n",
    "                          \", V0 \", '{:6.2f}'.format(V0),\n",
    "                          sep='')\n",
    "                else:\n",
    "                    episode_return.append(episode_cum_reward)\n",
    "                    print(\"Episode \", '{:2d}'.format(episode), \n",
    "                          \", epsilon \", '{:6.2f}'.format(epsilon), \n",
    "                          \", batch size \", '{:4d}'.format(len(self.replay_buffer)), \n",
    "                          \", ep return \", '{:4.1f}'.format(episode_cum_reward), \n",
    "                          sep='')\n",
    "\n",
    "                \n",
    "                state, _ = env.reset()\n",
    "                episode_cum_reward = 0\n",
    "            else:\n",
    "                state = next_state\n",
    "        return episode_return, MC_avg_discounted_reward, MC_avg_total_reward, V_init_state\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "        print(f\"Model saved in {path}\")\n",
    "\n",
    "    def load(self):\n",
    "        self.model.load_state_dict(torch.load(self.save_path, map_location=self.device))\n",
    "        self.target_network = deepcopy(self.model).to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "\n",
    "    def collect_sample(self,nb_sample):\n",
    "        s, _ = env.reset()\n",
    "        for _ in range(nb_sample):\n",
    "            a = self.act(s)\n",
    "            s2, r, done, trunc, _ = env.step(a)\n",
    "            self.replay_buffer.append(s, a, r, s2, done)\n",
    "            if done or trunc :\n",
    "                s, _ = env.reset()\n",
    "            else:\n",
    "                s = s2\n",
    "        print('end of collection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "Agent created\n",
      "end of collection\n",
      "Episode  1, epsilon   0.82, batch size 1200, ep return 10257663.3, MC tot 3432807.68, MC disc 115806.34, V0 34693.00\n",
      "Episode  2, epsilon   0.62, batch size 1400, ep return 8749501.2, MC tot 3432807.68, MC disc 115806.34, V0 46956.18\n",
      "Episode  3, epsilon   0.43, batch size 1600, ep return 8043320.0, MC tot 3335425.71, MC disc 171350.24, V0 61452.59\n",
      "Episode  4, epsilon   0.23, batch size 1800, ep return 9727052.6, MC tot 4290091.19, MC disc 200435.01, V0 81369.32\n",
      "Episode  5, epsilon   0.03, batch size 2000, ep return 11769147.7, MC tot 8907840.93, MC disc 291497.06, V0 95551.49\n",
      "Episode  6, epsilon   0.01, batch size 2200, ep return 30354961.1, MC tot 6787976.82, MC disc 81821.88, V0 114223.46\n",
      "Episode  7, epsilon   0.01, batch size 2400, ep return 7168858.9, MC tot 8861396.63, MC disc 104646.17, V0 190246.88\n",
      "Episode  8, epsilon   0.01, batch size 2600, ep return 27383832.8, MC tot 13476734.06, MC disc 184024.33, V0 249556.89\n",
      "Episode  9, epsilon   0.01, batch size 2800, ep return 16395109.7, MC tot 12527535.74, MC disc 184020.68, V0 274400.03\n",
      "Episode 10, epsilon   0.01, batch size 3000, ep return 22326997.4, MC tot 12093066.75, MC disc 183991.88, V0 298740.84\n",
      "Episode 11, epsilon   0.01, batch size 3200, ep return 12652283.5, MC tot 12227766.49, MC disc 170859.85, V0 300848.78\n",
      "Episode 12, epsilon   0.01, batch size 3400, ep return 18740387.8, MC tot 26977763.98, MC disc 182473.29, V0 286330.75\n",
      "Episode 13, epsilon   0.01, batch size 3600, ep return 15805269.5, MC tot 19534263.58, MC disc 139761.71, V0 268881.31\n",
      "Episode 14, epsilon   0.01, batch size 3800, ep return 26123211.8, MC tot 25335295.36, MC disc 184007.07, V0 273431.69\n",
      "Episode 15, epsilon   0.01, batch size 4000, ep return 40212773.2, MC tot 13972101.40, MC disc 185024.20, V0 244946.05\n",
      "Episode 16, epsilon   0.01, batch size 4200, ep return 37453468.9, MC tot 17526900.68, MC disc 292832.90, V0 242723.23\n",
      "Episode 17, epsilon   0.01, batch size 4400, ep return 37215154.5, MC tot 17221364.22, MC disc 284545.04, V0 215863.16\n",
      "Episode 18, epsilon   0.01, batch size 4600, ep return 30580558.5, MC tot 17010886.81, MC disc 288167.30, V0 233391.67\n",
      "Episode 19, epsilon   0.01, batch size 4800, ep return 36532158.8, MC tot 16015590.54, MC disc 257811.75, V0 226755.78\n",
      "Episode 20, epsilon   0.01, batch size 5000, ep return 31029090.7, MC tot 12784228.45, MC disc 185076.15, V0 192689.25\n",
      "Episode 21, epsilon   0.01, batch size 5200, ep return 27934490.0, MC tot 12784228.45, MC disc 185076.15, V0 210214.92\n",
      "Episode 22, epsilon   0.01, batch size 5400, ep return 22149608.3, MC tot 15778688.45, MC disc 208278.64, V0 173792.42\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAgent created\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m agent\u001b[38;5;241m.\u001b[39mcollect_sample(\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m agent\u001b[38;5;241m.\u001b[39mtrain(env, \u001b[38;5;241m500\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 135\u001b[0m, in \u001b[0;36mProjectAgent.train\u001b[0;34m(self, env, max_episode)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# Monitoring\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmonitoring_nb_trials\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 135\u001b[0m     MC_dr, MC_tr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMC_eval(env, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmonitoring_nb_trials)    \u001b[38;5;66;03m# NEW NEW NEW\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     V0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mV_initial_state(env, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmonitoring_nb_trials)   \u001b[38;5;66;03m# NEW NEW NEW\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     MC_avg_total_reward\u001b[38;5;241m.\u001b[39mappend(MC_tr)   \u001b[38;5;66;03m# NEW NEW NEW\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 58\u001b[0m, in \u001b[0;36mProjectAgent.MC_eval\u001b[0;34m(self, env, nb_trials)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (done \u001b[38;5;129;01mor\u001b[39;00m trunc):\n\u001b[1;32m     57\u001b[0m     a \u001b[38;5;241m=\u001b[39m greedy_action(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, x)\n\u001b[0;32m---> 58\u001b[0m     y,r,done,trunc,_ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(a)\n\u001b[1;32m     59\u001b[0m     x \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m     60\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m r\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/gymnasium/wrappers/common.py:125\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    114\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/MVA/mva-rl-assignment-antoine-martinez/src/env_hiv.py:231\u001b[0m, in \u001b[0;36mHIVPatient.step\u001b[0;34m(self, a_index)\u001b[0m\n\u001b[1;32m    229\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate()\n\u001b[1;32m    230\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_set[a_index]\n\u001b[0;32m--> 231\u001b[0m state2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransition(state, action, \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m    232\u001b[0m rew \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward(state, action, state2)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclipping:\n",
      "File \u001b[0;32m~/MVA/mva-rl-assignment-antoine-martinez/src/env_hiv.py:216\u001b[0m, in \u001b[0;36mHIVPatient.transition\u001b[0;34m(self, state, action, duration)\u001b[0m\n\u001b[1;32m    213\u001b[0m     state1 \u001b[38;5;241m=\u001b[39m state0 \u001b[38;5;241m+\u001b[39m der \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1e-3\u001b[39m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# np.clip(state1, self.lower, self.upper, out=state1)\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m     state0 \u001b[38;5;241m=\u001b[39m state1\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state1\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seed_everything()\n",
    "print('Start')\n",
    "model = DQN(6, 4)\n",
    "agent = ProjectAgent(env, model)\n",
    "print('Agent created')\n",
    "agent.collect_sample(1000)\n",
    "agent.train(env, 500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
